{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GRodrigues4/rPPG-10/blob/main/rPPG_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6S6AsfejDxc"
      },
      "source": [
        "### Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Qw7mQsYqx6MQ"
      },
      "outputs": [],
      "source": [
        "# Install all the needed packages\n",
        "!pip install mediapipe\n",
        "\n",
        "import mediapipe as mp\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "from mediapipe import solutions\n",
        "from mediapipe.framework.formats import landmark_pb2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bjnE6lbmjIA9"
      },
      "outputs": [],
      "source": [
        "# Import all the necessary libraries\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import scipy.fftpack as fftpack\n",
        "import scipy.signal as signal\n",
        "import scipy.interpolate as interpolate\n",
        "from scipy import stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbpPChsnyVbC"
      },
      "outputs": [],
      "source": [
        "# Give access to the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Import the Mediapipe model\n",
        "!wget -O face_landmarker_v2_with_blendshapes.task -q https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lU2gJNY-i2Dm"
      },
      "source": [
        "# Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qrI1-2M_8AC-"
      },
      "outputs": [],
      "source": [
        "class SignalProcessing:\n",
        "  def __init__(self, file, sample_rate = 1000):\n",
        "\n",
        "    # Constructor to initialize the SignalProcessing class with parameters:\n",
        "    # file: The file name containing the data.\n",
        "    # sample_rate: The rate at which data is sampled, in Hz (samples per second).\n",
        "\n",
        "    self.file_name = file\n",
        "    self.sample_rate = sample_rate\n",
        "\n",
        "\n",
        "  def cut(self):\n",
        "\n",
        "    # Method to preprocess the data by selecting a portion of the ECG signal\n",
        "    # based on the condition that there is no '1' found in a specific range of 'In' values.\n",
        "    # Returns the preprocessed ECG and Input (button for sync) signals.\n",
        "\n",
        "    data = np.loadtxt(self.file_name) # Load the data from the file.\n",
        "    ECG = []  # Initialize an empty list to store ECG signal values.\n",
        "    In = []   # Initialize an empty list to store 'In' signal values.\n",
        "\n",
        "    # Iterate through each row in the data.\n",
        "    for row in data:\n",
        "      ECG.append(row[5])  # Append the 6th column (ECG) to the ECG list.\n",
        "      In.append(row[1])   # Append the 2nd column (In) to the In list.\n",
        "\n",
        "    # Iterate through ECG data in steps of 100.\n",
        "    for index in range(0,len(ECG),50):\n",
        "      # Check if there is no '1' in the 'In' signal within the specified range.\n",
        "      if not any(element == 1 for element in In[index:(index+60*self.sample_rate)]):\n",
        "        # Select the ECG and In data for the next 10 minutes (600 seconds).\n",
        "        ECG = ECG[index:(index+60*10*self.sample_rate)]\n",
        "        In = In[index:(index+60*10*self.sample_rate)]\n",
        "        break # Exit the loop once the desired segment is found.\n",
        "\n",
        "    return ECG\n",
        "\n",
        "\n",
        "  def run(self, output_file = \"output_file.txt\"):\n",
        "\n",
        "    # Method to run the complete processing pipeline: cutting the data, calculating HR, and saving the results.\n",
        "\n",
        "    ECG = self.cut()  # Cut and preprocess the data.\n",
        "    np.save(output_file + '_ECG', ECG)  # Save the ECG signal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "x7P7LP0bnPXE"
      },
      "outputs": [],
      "source": [
        "class VideoSegmentation:\n",
        "  def __init__(self, file_path, fps = 30, image_width = 1280,\n",
        "               image_height = 720, bounding_box_size = 64):\n",
        "\n",
        "    # Constructor to initialize the VideoSegmentation class with parameters:\n",
        "    # file_path: Path to the video file.\n",
        "    # fps: Frames per second of the video.\n",
        "    # image_width: Width of the video frame in pixels.\n",
        "    # image_height: Height of the video frame in pixels.\n",
        "    # bounding_box_size: Size of the bounding box around facial landmarks.\n",
        "    # coordinates: List to store previous ROI location in case of blurry frame\n",
        "\n",
        "    self.file_path = file_path\n",
        "    self.fps = fps\n",
        "    self.image_width = image_width\n",
        "    self.image_height = image_height\n",
        "    self.box_size = bounding_box_size\n",
        "    self.coordinates = []\n",
        "\n",
        "\n",
        "  def relative_to_absolute(self, relative_coords):\n",
        "\n",
        "    # Convert relative coordinates (normalized between 0 and 1) to absolute pixel values.\n",
        "    # Returns a list of absolute coordinates.\n",
        "\n",
        "    absolute_coords = [[int(coord[0] * self.image_width), int(coord[1] * self.image_height)] for coord in relative_coords]\n",
        "\n",
        "    return absolute_coords\n",
        "\n",
        "\n",
        "  def segment_frame(self, frame, time_ms):\n",
        "\n",
        "    # Method to detect facial landmarks in the given frame and return the coordinates of specific landmarks.\n",
        "    # Returns a list of relative coordinates for the specified landmarks.\n",
        "\n",
        "    # Create a face landmarker instance with the video mode in order to extract the facial features\n",
        "    base_options = python.BaseOptions(model_asset_path = 'face_landmarker_v2_with_blendshapes.task')\n",
        "    options = vision.FaceLandmarkerOptions(base_options = base_options, running_mode = mp.tasks.vision.RunningMode.VIDEO)\n",
        "    detector = vision.FaceLandmarker.create_from_options(options)\n",
        "\n",
        "    # Convert the frame to RGB as the model expects RGB input.\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_rgb)\n",
        "\n",
        "    # Detect facial landmarks for the frame at the given time in milliseconds.\n",
        "    result = detector.detect_for_video(mp_image, round(time_ms))\n",
        "\n",
        "    # Extract specific landmark coordinates: forehead, cheek 1, and cheek 2.\n",
        "    coord = [(result.face_landmarks[0][151].x, result.face_landmarks[0][151].y),(result.face_landmarks[0][50].x, result.face_landmarks[0][50].y)\n",
        "          ,(result.face_landmarks[0][280].x, result.face_landmarks[0][280].y)]\n",
        "    coord = self.relative_to_absolute(coord)\n",
        "\n",
        "    return coord\n",
        "\n",
        "\n",
        "  def create_bounding_box(self, coord):\n",
        "\n",
        "    # Method to create a bounding box around a given coordinate.\n",
        "    # Returns the top-left and bottom-right coordinates of the bounding box.\n",
        "\n",
        "    x_coord = coord[0]\n",
        "    y_coord = coord[1]\n",
        "\n",
        "    # Calculate the bounding box limits ensuring they stay within frame boundaries.\n",
        "    x_min = max(0, x_coord - self.box_size // 2)\n",
        "    y_min = max(0, 5+(y_coord - self.box_size // 2))\n",
        "    x_max = min(self.image_width, x_coord + self.box_size // 2)\n",
        "    y_max = min(self.image_height, 5+(y_coord + self.box_size // 2))\n",
        "\n",
        "    return (int(x_min), int(y_min)), (int(x_max), int(y_max))\n",
        "\n",
        "\n",
        "  def cut_video(self, frame, coord):\n",
        "\n",
        "    # Method to extract a specific segment of the video frame using a bounding box.\n",
        "    # Returns the segmented frame.\n",
        "\n",
        "    top_left, bottom_right = self.create_bounding_box(coord)\n",
        "    segmented_frame = frame[top_left[1]:bottom_right[1], top_left[0]:bottom_right[0]]\n",
        "\n",
        "    return segmented_frame\n",
        "\n",
        "\n",
        "  def run(self, output_path = \"output_video\"):\n",
        "\n",
        "    # Method to execute the complete video segmentation process and save the results.\n",
        "\n",
        "    # Open the video file using OpenCV.\n",
        "    cap = cv2.VideoCapture(self.file_path)\n",
        "    if not cap.isOpened():\n",
        "      raise ValueError(\"Error opening video file\")\n",
        "\n",
        "    interval_video_F = []   # List to store segmented frames for the forehead.\n",
        "    interval_video_C1 = []  # List to store segmented frames for the first cheek.\n",
        "    interval_video_C2 = []  # List to store segmented frames for the second cheek.\n",
        "\n",
        "    # Define the codec and create a VideoWriter object for each region.\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "    writerF = cv2.VideoWriter(output_path + '_Forehead_.avi', fourcc, self.fps, (64, 64), 1)\n",
        "    writerC1 = cv2.VideoWriter(output_path + '_Cheek1_.avi', fourcc, self.fps, (64, 64), 1)\n",
        "    writerC2 = cv2.VideoWriter(output_path + '_Cheek2_.avi', fourcc, self.fps, (64, 64), 1)\n",
        "\n",
        "    # Process each frame of the video.\n",
        "    while cap.isOpened():\n",
        "      ret, frame = cap.read()\n",
        "      time_ms = cap.get(cv2.CAP_PROP_POS_MSEC)  # Get the current frame timestamp in milliseconds.\n",
        "\n",
        "      # Check if the video has ended.\n",
        "      if not ret:\n",
        "        raise ValueError(\"Error reading frame\")\n",
        "        break\n",
        "\n",
        "      # Resize the frame if its dimensions don't match the expected ones.\n",
        "      if int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) != self.image_height:\n",
        "        frame = cv2.resize(frame, (self.image_width, self.image_height))\n",
        "      try:\n",
        "        # Detect and get facial landmark coordinates.\n",
        "        self.coordinates = self.segment_frame(frame, time_ms)\n",
        "      except IndexError as e:\n",
        "        continue\n",
        "\n",
        "      writerF.write(cv2.convertScaleAbs(np.array(self.cut_video(frame, self.coordinates[0]))))\n",
        "      writerC1.write(cv2.convertScaleAbs(np.array(self.cut_video(frame, self.coordinates[1]))))\n",
        "      writerC2.write(cv2.convertScaleAbs(np.array(self.cut_video(frame, self.coordinates[2]))))\n",
        "\n",
        "      # Print the progress for every 1000th frame.\n",
        "      if cap.get(cv2.CAP_PROP_POS_FRAMES) % 1000 == 0:\n",
        "        print(f\"Frame {cap.get(cv2.CAP_PROP_POS_FRAMES)}\")\n",
        "\n",
        "      # Break the loop if the video exceeds 10 minutes.\n",
        "      if cap.get(cv2.CAP_PROP_POS_FRAMES) == (self.fps*60*10):\n",
        "        break\n",
        "\n",
        "    print(output_path + '. . . . . . . Done\\n')\n",
        "\n",
        "    # Release the video capture and close any OpenCV windows.\n",
        "    writerF.release()\n",
        "    writerC1.release()\n",
        "    writerC2.release()\n",
        "    cap.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcVfPn_6mNAN"
      },
      "outputs": [],
      "source": [
        "def CreateDataset(array_like):\n",
        "\n",
        "  # Function that applies both the Video and Signal processing on the raw dataset.\n",
        "  # Saves the interval videos and a txt file with the HR groundtruth.\n",
        "\n",
        "  # Iterate through the given list.\n",
        "  for subject_num in array_like:\n",
        "    in_path = '/content/drive/My Drive/Data Tese/Subject_' + str(subject_num) + '/Subject_' + str(subject_num)  # Define file paths.\n",
        "    out_path = '/content/drive/My Drive/Dataset_rPPG-10/Subject_' + str(subject_num) + '/Subject_' + str(subject_num)\n",
        "\n",
        "    # Apply image and signal processing functions defined previously\n",
        "    SignalProcessing(in_path + '.txt').run(output_file = out_path)\n",
        "    VideoSegmentation(in_path + '.mp4').run(output_path = out_path)\n",
        "\n",
        "# Create a list of all the subjects to be processed.\n",
        "List = [4]\n",
        "\n",
        "CreateDataset(List)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "_6S6AsfejDxc",
        "lU2gJNY-i2Dm",
        "jnM_mQjpxmQ2",
        "-IKJsgwtDz0m",
        "xDdujzJlxNsb",
        "0iGvw7CAxa4U",
        "EVHAJjJnyjtt",
        "a6tvFqUvMTgC",
        "Rj7DYlDcxU-t",
        "jFTbly5ND-IG",
        "M61J6EGExfjr",
        "B4S_hpZSxklI",
        "ilO6zaSDEDiJ"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}